{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716e3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('TkAgg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "img_path = 'imgs/'\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da73b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Chars:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.char2index = {}\n",
    "        self.char2count = {}\n",
    "        self.index2char = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_chars = 2\n",
    "\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.index2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f5a8a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readChars(lang1, lang2, reverse=False):\n",
    "    path = lang1 + '-' + lang2\n",
    "    data_path = 'data/'+path+'.csv'\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    source_words = []\n",
    "    target_words = []\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        pair = df.iloc[i][[lang1, lang2]]\n",
    "        pairs.append(pair)\n",
    "        source_words.append(pair[0])\n",
    "        target_words.append(pair[1])\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Chars(lang2)\n",
    "        output_lang = Chars(lang1)\n",
    "    else:\n",
    "        input_lang = Chars(lang1)\n",
    "        output_lang = Chars(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6628e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readChars(lang1, lang2, reverse)\n",
    "    print(\"Read {} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting chars...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addWord(pair[0])\n",
    "        output_lang.addWord(pair[1])\n",
    "    print(\"Counted chars:\")\n",
    "    print(input_lang.name, input_lang.n_chars)\n",
    "    print(output_lang.name, output_lang.n_chars)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1ce842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 350 sentence pairs\n",
      "Counting chars...\n",
      "Counted chars:\n",
      "fin 27\n",
      "ina 38\n",
      "['lisæːntyæ', 'lɑsɑnið']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('ina', 'fin', reverse=True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a622cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH_INPUT = max(len(pair[0]) for pair in pairs)\n",
    "MAX_LENGTH_OUTPUT = max(len(pair[1]) for pair in pairs)\n",
    "MAX_LENGTH = max(MAX_LENGTH_INPUT, MAX_LENGTH_OUTPUT)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1a3f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bdfb9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5d710ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) \n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "73a91bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromWord(lang, word):\n",
    "    return [lang.char2index[char] for char in word]\n",
    "\n",
    "def tensorFromWord(lang, word):\n",
    "    indexes = indexesFromWord(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromWord(input_lang, pair[0])\n",
    "    target_tensor = tensorFromWord(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('ina', 'fin')\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromWord(input_lang, inp)\n",
    "        tgt_ids = indexesFromWord(output_lang, tgt)\n",
    "        \n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    all_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_test_gen = torch.Generator().manual_seed(42)\n",
    "    train_data, test_data = random_split(all_data, [0.8, 0.2], generator=train_test_gen)\n",
    "    \n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    test_sampler = RandomSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    return input_lang, output_lang, train_dataloader, test_dataloader, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3daed1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1734827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ff758fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(encoder, decoder, epoch, hidden_size, batch_size, learning_rate, final=False):\n",
    "    if final:\n",
    "        torch.save(encoder.state_dict(), 'ckpt/encoder_'+str(epoch)+'_'+str(hidden_size)+'_'+str(batch_size)+'_'+str(learning_rate)+'_final.dict')\n",
    "        torch.save(decoder.state_dict(), 'ckpt/decoder_'+str(epoch)+'_'+str(hidden_size)+'_'+str(batch_size)+'_'+str(learning_rate)+'_final.dict')\n",
    "    else:    \n",
    "        torch.save(encoder.state_dict(), 'ckpt/encoder_'+str(epoch)+'_'+str(hidden_size)+'_'+str(batch_size)+'_'+str(learning_rate)+'.dict')\n",
    "        torch.save(decoder.state_dict(), 'ckpt/decoder_'+str(epoch)+'_'+str(hidden_size)+'_'+str(batch_size)+'_'+str(learning_rate)+'.dict')\n",
    "    \n",
    "def resume(epoch, hidden_size, batch_size, learning_rate, dropout=0.1):\n",
    "    encoder = EncoderRNN(input_lang.n_chars, hidden_size).to(device)\n",
    "    decoder = AttnDecoderRNN(hidden_size, output_lang.n_chars, dropout=dropout).to(device)\n",
    "\n",
    "    encoder.load_state_dict(torch.load('ckpt/encoder_'+str(epoch)+'_'+str(hidden_size)+'_'+str(batch_size)+'_'+str(learning_rate)+'.dict'))\n",
    "    decoder.load_state_dict(torch.load('ckpt/decoder_'+str(epoch)+'_'+str(hidden_size)+'_'+str(batch_size)+'_'+str(learning_rate)+'.dict'))\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c0cc71ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:04:50] Energy consumed for RAM : 0.004363 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 15:04:50] Energy consumed for all CPUs : 0.025376 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 15:04:50] 0.029739 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder = resume(15, 512, 64, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8ba8498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_decreasing_with_patience(lst, patience=5):\n",
    "    non_decreasing_count = 0\n",
    "\n",
    "    for i in range(len(lst) - 1):\n",
    "        if lst[i] <= lst[i + 1]:\n",
    "            non_decreasing_count += 1\n",
    "            if non_decreasing_count > patience:\n",
    "                return False\n",
    "        else:\n",
    "            non_decreasing_count = 0\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e7c5733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, test_dataloader, encoder, decoder, n_epochs, learning_rate,\n",
    "          hidden_size, batch_size, patience=5):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 \n",
    "    plot_loss_total = 0 \n",
    "    \n",
    "    val_losses = []\n",
    "    validation_list = []\n",
    "    print_val_loss_total = 0\n",
    "    plot_val_loss_total = 0\n",
    "    bleu_total = []\n",
    "    chrf_total = []\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        start = time.time()\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        checkpoint(encoder, decoder, epoch, hidden_size, batch_size, learning_rate)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        val_loss = validate(test_dataloader, encoder, decoder)\n",
    "\n",
    "        validation_list.append(val_loss)\n",
    "        \n",
    "        print_val_loss_total += val_loss\n",
    "        plot_val_loss_total += val_loss\n",
    "        bleu, chrf = calculate_scores(test_data)\n",
    "\n",
    "        bleu_total.append(bleu)\n",
    "        chrf_total.append(chrf)\n",
    "\n",
    "        print_loss_avg = print_loss_total\n",
    "        print_loss_total = 0\n",
    "            \n",
    "        print_val_loss_avg = print_val_loss_total\n",
    "        print_val_loss_total = 0\n",
    "            \n",
    "        print('Epoch: {}/{},\\tTime Taken: {:.2f} seconds,\\tTraining Loss: {:.4f},\\tValidation Loss: {:.4f}'.format(epoch, n_epochs, time.time()-start, print_loss_avg, print_val_loss_avg))\n",
    "        \n",
    "        plot_loss_avg = plot_loss_total\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "            \n",
    "        plot_val_loss_avg = plot_val_loss_total\n",
    "        val_losses.append(plot_val_loss_avg)\n",
    "        plot_val_loss_total = 0\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement == patience:\n",
    "            print(\"Validation loss has not gone down for \" + str(patience) + \" epochs. Implementing Early Stopping\")\n",
    "            encoder, decoder = resume(validation_list.index(best_val_loss)+1, hidden_size, batch_size, learning_rate) #get the epoch needed\n",
    "            checkpoint(encoder, decoder, validation_list.index(best_val_loss)+1, hidden_size, batch_size, learning_rate, final=True)\n",
    "            break\n",
    "\n",
    "    showPlot(plot_losses, val_losses, hidden_size, batch_size, learning_rate)\n",
    "    showMetricPlot(bleu_total, chrf_total, hidden_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f3a99c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(test_dataloader, encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data in test_dataloader:\n",
    "    \n",
    "        input_tensor, target_tensor = data\n",
    "    \n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor) \n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    return total_loss/len(test_dataloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df4fa016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(train_loss, val_loss, hs, bs, lr):\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    ax.plot(train_loss, label='Training Loss', color='blue')\n",
    "    ax.plot(val_loss, label='Validation Loss',color='red')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Training and Validation Loss Curve\")\n",
    "    plt.savefig('imgs/Train_Val_Loss_Graph'+str(hs)+'_'+str(bs)+'_'+str(lr)+'.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "def showMetricPlot(b, c, hs, bs, lr):\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    ax.plot(b, label='Bleu', color='green')\n",
    "    ax.plot(c, label='CHRF', color='orange')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    #ax.set_ylabel('Score')\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Bleu and CHRF Scores while Training\")\n",
    "    plt.savefig('imgs/Bleu_CHRF_Score_Graph'+str(hs)+'_'+str(bs)+'_'+str(lr)+'.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "73b7f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluater(encoder, decoder, word, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        #input_tensor = tensorFromWord(input_lang, word)\n",
    "        input_tensor = word\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_chars = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_chars.append('<EOS>')\n",
    "                break\n",
    "            decoded_chars.append(output_lang.index2char[idx.item()])\n",
    "        \n",
    "    return decoded_chars, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "98802f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_word, output_chars, correct_output, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone', extent=[-0.5, len(input_word)-0.5, len(output_chars)-0.5, -0.5])\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_title(input_word + ' -> ' + correct_output)\n",
    "    \n",
    "    ax.set_xticklabels([''] + [*input_word] + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_chars)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.savefig(img_path+input_word+'_attention.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3eb8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10, eval_all=False):\n",
    "    if eval_all:\n",
    "        for i, pair in enumerate(test_data):\n",
    "            pair = (torch.unsqueeze(pair[0], 0), torch.unsqueeze(pair[1], 0))\n",
    "            output_chars, attentions = evaluater(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "            output_word = ''.join(output_chars)\n",
    "            decoded_input = []\n",
    "        \n",
    "            in0=[]\n",
    "            for idx in torch.squeeze(pair[0],0):\n",
    "                if idx.item() == EOS_token:\n",
    "                    in0.append('<EOS>')\n",
    "                    break\n",
    "                in0.append(input_lang.index2char[idx.item()])\n",
    "            \n",
    "            in1=[]\n",
    "            for idx in torch.squeeze(pair[1],0):\n",
    "                if idx.item() == EOS_token:\n",
    "                    in1.append('<EOS>')\n",
    "                    break\n",
    "                in1.append(output_lang.index2char[idx.item()])\n",
    "            \n",
    "            input_word = ''.join(decoded_input)\n",
    "            print('>', ''.join(in0))\n",
    "            print('=', ''.join(in1))\n",
    "            print('<', output_word)\n",
    "            print('')\n",
    "            showAttention(''.join(in0[0:-1]), output_chars, ''.join(in1[0:-1]), attentions[0, :len(output_chars), :])\n",
    "    for i in range(n):\n",
    "        random_index = random.randint(0, len(test_data))\n",
    "        pair = test_data.dataset[random_index]\n",
    "        pair = (torch.unsqueeze(pair[0], 0), torch.unsqueeze(pair[1], 0))\n",
    "        \n",
    "        output_chars, attentions = evaluater(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_word = ''.join(output_chars)\n",
    "        decoded_input = []\n",
    "        \n",
    "        in0=[]\n",
    "        for idx in torch.squeeze(pair[0],0):\n",
    "            if idx.item() == EOS_token:\n",
    "                in0.append('<EOS>')\n",
    "                break\n",
    "            in0.append(input_lang.index2char[idx.item()])\n",
    "            \n",
    "        in1=[]\n",
    "        for idx in torch.squeeze(pair[1],0):\n",
    "            if idx.item() == EOS_token:\n",
    "                in1.append('<EOS>')\n",
    "                break\n",
    "            in1.append(output_lang.index2char[idx.item()])\n",
    "            \n",
    "        input_word = ''.join(decoded_input)\n",
    "        print('>', ''.join(in0))\n",
    "        print('=', ''.join(in1))\n",
    "        print('<', output_word)\n",
    "        print('')\n",
    "        showAttention(''.join(in0[0:-1]), output_chars, ''.join(in1[0:-1]), attentions[0, :len(output_chars), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f10dbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def modified_precision(candidate, references, n):\n",
    "\n",
    "    cand_counts = Counter(candidate)\n",
    "    max_ref_counts = Counter(references)\n",
    "\n",
    "    for ref in references:\n",
    "        max_ref_counts |= Counter(ref)\n",
    "\n",
    "    clipped_counts = {ngram: min(count, max_ref_counts[ngram]) for ngram, count in cand_counts.items()}\n",
    "\n",
    "    return sum(clipped_counts.values()) / (len(candidate) or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9e18231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:23:19] Energy consumed for RAM : 0.000291 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 13:23:19] Energy consumed for all CPUs : 0.001692 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 13:23:19] 0.001982 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(s, n):\n",
    "    if len(s) < n:\n",
    "        return []\n",
    "\n",
    "    ngrams = [s[i:i+n] for i in range(len(s)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3e236041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({1: 0.6191984998215584, 2: 0.2889592999167272, 3: 0.0, 4: 0.0}, 0.2270394499345714)\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "import math\n",
    "\n",
    "def bleu_score(can, ref, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    scores = {}\n",
    "    for which_grammy in range(1, len(weights) + 1):\n",
    "        scores_temp = []\n",
    "        canngrams = generate_ngrams(can, which_grammy)\n",
    "        refngrams = generate_ngrams(ref, which_grammy)\n",
    "        bp = math.e**(1 - (len(ref) / len(can)))\n",
    "        scores[which_grammy] = bp * modified_precision(canngrams, refngrams, which_grammy)\n",
    "    total_score = 0\n",
    "    for score, weight in zip(scores, weights):\n",
    "        total_score = total_score + scores[score] * weight\n",
    "    return scores, total_score\n",
    "\n",
    "print(bleu_score('aaabdhd','abcdaaks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1c65645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance_and_alignment(s, t):\n",
    "    rows, cols = len(s) + 1, len(t) + 1\n",
    "    dist = [[0 for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "    for i in range(1, rows):\n",
    "        dist[i][0] = i\n",
    "    for j in range(1, cols):\n",
    "        dist[0][j] = j\n",
    "\n",
    "    for i in range(1, rows):\n",
    "        for j in range(1, cols):\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            dist[i][j] = min(dist[i - 1][j] + 1,\n",
    "                             dist[i][j - 1] + 1,\n",
    "                             dist[i - 1][j - 1] + cost)\n",
    "\n",
    "    aligned_s, aligned_t = \"\", \"\"\n",
    "    i, j = len(s), len(t)\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and j > 0 and dist[i][j] == dist[i - 1][j - 1] + (s[i - 1] != t[j - 1]):\n",
    "            aligned_s = s[i - 1] + aligned_s\n",
    "            aligned_t = t[j - 1] + aligned_t\n",
    "            i, j = i - 1, j - 1\n",
    "        elif i > 0 and dist[i][j] == dist[i - 1][j] + 1:\n",
    "            aligned_s = s[i - 1] + aligned_s\n",
    "            aligned_t = \"$\" + aligned_t\n",
    "            i -= 1\n",
    "        else:\n",
    "            aligned_s = \"#\" + aligned_s\n",
    "            aligned_t = t[j - 1] + aligned_t\n",
    "            j -= 1\n",
    "\n",
    "    return aligned_s, aligned_t, dist[len(s)][len(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c7e98812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chrf(reference, candidate, max_n=6, beta=2):\n",
    "    def ngrams(s, n):\n",
    "        return [s[i:i+n] for i in range(len(s) - n + 1)]\n",
    "\n",
    "    def count_ngrams(ngram_list):\n",
    "        ngram_count = {}\n",
    "        for ngram in ngram_list:\n",
    "            ngram_count[ngram] = ngram_count.get(ngram, 0) + 1\n",
    "        return ngram_count\n",
    "\n",
    "    chrp_total = 0\n",
    "    chrr_total = 0\n",
    "\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_ngrams = count_ngrams(ngrams(reference, n))\n",
    "        cand_ngrams = count_ngrams(ngrams(candidate, n))\n",
    "\n",
    "        overlap = sum(min(ref_ngrams.get(ng, 0), cand_ngrams.get(ng, 0)) for ng in cand_ngrams)\n",
    "\n",
    "        chrp = overlap / sum(cand_ngrams.values()) if cand_ngrams else 0\n",
    "        chrr = overlap / sum(ref_ngrams.values()) if ref_ngrams else 0\n",
    "\n",
    "        chrp_total += chrp\n",
    "        chrr_total += chrr\n",
    "\n",
    "    avg_chrp = chrp_total / max_n\n",
    "    avg_chrr = chrr_total / max_n\n",
    "\n",
    "    if avg_chrp + avg_chrr == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (1 + beta**2) * (avg_chrp * avg_chrr) / (beta**2 * avg_chrp + avg_chrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7f00bc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.30184083777607607, 0.3121011564380961)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:28:59] Energy consumed for RAM : 0.007442 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 16:28:59] Energy consumed for all CPUs : 0.043283 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 16:28:59] 0.050725 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:28:59] Energy consumed for RAM : 0.007738 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 16:28:59] Energy consumed for all CPUs : 0.045008 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 16:28:59] 0.052747 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "def calculate_scores(test_data):\n",
    "    total = 0\n",
    "    length = len(test_data)\n",
    "    total_bleu_1 = 0\n",
    "    total_chrf = 0\n",
    "    for pair in test_data:\n",
    "        pair = (torch.unsqueeze(pair[0], 0), torch.unsqueeze(pair[1], 0))\n",
    "        \n",
    "        output_chars, _ = evaluater(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        \n",
    "        in1 = []\n",
    "        for idx in torch.squeeze(pair[1], 0):\n",
    "            if idx.item() == EOS_token:\n",
    "                break\n",
    "            in1.append(output_lang.index2char[idx.item()])\n",
    "\n",
    "        ref_string = ''.join(in1)\n",
    "        candidate_string = ''.join(output_chars[:-1])\n",
    "\n",
    "        r, c, _ = levenshtein_distance_and_alignment(ref_string, candidate_string)\n",
    "        \n",
    "        scores, total_score = bleu_score(candidate_string, ref_string)\n",
    "        chrf_temp = chrf(ref_string, candidate_string, max_n=4)\n",
    "        total_bleu_1 = total_bleu_1 + scores[1]\n",
    "        total = total + total_score\n",
    "        total_chrf = total_chrf + chrf_temp\n",
    "    return total / length, total_chrf / length\n",
    "\n",
    "print(calculate_scores(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8b050103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:23:23] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 13:23:23] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 13:23:23] No GPU found.\n",
      "[codecarbon INFO @ 13:23:23] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 13:23:23] No CPU tracking mode found. Falling back on CPU constant mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 350 sentence pairs\n",
      "Counting chars...\n",
      "Counted chars:\n",
      "ina 38\n",
      "fin 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:23:24] CPU Model on constant consumption mode: 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz\n",
      "[codecarbon INFO @ 13:23:24] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 13:23:24]   Platform system: Linux-6.1.55-06877-gc83437f2949f-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 13:23:24]   Python version: 3.9.2\n",
      "[codecarbon INFO @ 13:23:24]   CodeCarbon version: 2.3.2\n",
      "[codecarbon INFO @ 13:23:24]   Available RAM : 6.421 GB\n",
      "[codecarbon INFO @ 13:23:24]   CPU count: 8\n",
      "[codecarbon INFO @ 13:23:24]   CPU model: 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz\n",
      "[codecarbon INFO @ 13:23:24]   GPU count: None\n",
      "[codecarbon INFO @ 13:23:24]   GPU model: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000,\tTime Taken: 4.80 seconds,\tTraining Loss: 1.2583,\tValidation Loss: 1.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:23:34] Energy consumed for RAM : 0.000301 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 13:23:34] Energy consumed for all CPUs : 0.001750 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 13:23:34] 0.002051 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/1000,\tTime Taken: 4.52 seconds,\tTraining Loss: 0.8209,\tValidation Loss: 0.8219\n",
      "Epoch: 3/1000,\tTime Taken: 4.46 seconds,\tTraining Loss: 0.5757,\tValidation Loss: 0.7502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:23:42] Energy consumed for RAM : 0.000010 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 13:23:42] Energy consumed for all CPUs : 0.000058 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 13:23:42] 0.000068 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/1000,\tTime Taken: 4.22 seconds,\tTraining Loss: 0.3934,\tValidation Loss: 0.6593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:23:49] Energy consumed for RAM : 0.000311 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 13:23:49] Energy consumed for all CPUs : 0.001808 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 13:23:49] 0.002119 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/1000,\tTime Taken: 4.55 seconds,\tTraining Loss: 0.2500,\tValidation Loss: 0.6585\n",
      "Epoch: 6/1000,\tTime Taken: 4.09 seconds,\tTraining Loss: 0.1639,\tValidation Loss: 0.6612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:23:57] Energy consumed for RAM : 0.000020 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 13:23:57] Energy consumed for all CPUs : 0.000117 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 13:23:57] 0.000137 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/1000,\tTime Taken: 4.36 seconds,\tTraining Loss: 0.1097,\tValidation Loss: 0.6419\n",
      "Epoch: 8/1000,\tTime Taken: 4.74 seconds,\tTraining Loss: 0.0731,\tValidation Loss: 0.6896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:24:04] Energy consumed for RAM : 0.000321 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 13:24:04] Energy consumed for all CPUs : 0.001867 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 13:24:04] 0.002188 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/1000,\tTime Taken: 4.68 seconds,\tTraining Loss: 0.0442,\tValidation Loss: 0.6990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:24:12] Energy consumed for RAM : 0.000030 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 13:24:12] Energy consumed for all CPUs : 0.000175 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 13:24:12] 0.000205 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/1000,\tTime Taken: 4.98 seconds,\tTraining Loss: 0.0266,\tValidation Loss: 0.7181\n",
      "Epoch: 11/1000,\tTime Taken: 4.72 seconds,\tTraining Loss: 0.0170,\tValidation Loss: 0.7337\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m tracker \u001b[38;5;241m=\u001b[39m EmissionsTracker(output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memissions\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memissions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m tracker\u001b[38;5;241m.\u001b[39mstart()  \u001b[38;5;66;03m# Start tracking\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m emissions \u001b[38;5;241m=\u001b[39m tracker\u001b[38;5;241m.\u001b[39mstop()  \u001b[38;5;66;03m# Stop tracking and get emissions\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmissions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memissions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m kg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[77], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, test_dataloader, encoder, decoder, n_epochs, learning_rate, hidden_size, batch_size, patience)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     23\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 24\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     checkpoint(encoder, decoder, epoch, hidden_size, batch_size, learning_rate)\n\u001b[1;32m     26\u001b[0m     print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[72], line 17\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     11\u001b[0m decoder_outputs, _, _ \u001b[38;5;241m=\u001b[39m decoder(encoder_outputs, encoder_hidden, target_tensor)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     14\u001b[0m     decoder_outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, decoder_outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     15\u001b[0m     target_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 512\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "dropout = 0.1\n",
    "\n",
    "input_lang, output_lang, train_dataloader, test_dataloader, test_data = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_chars, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_chars, dropout=dropout).to(device)\n",
    "\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "tracker = EmissionsTracker(output_dir=\"emissions\", output_file=\"emissions.csv\")\n",
    "\n",
    "tracker.start()\n",
    "\n",
    "train(train_dataloader, test_dataloader, encoder, decoder, 1000, learning_rate=learning_rate, patience=5, hidden_size=hidden_size, batch_size=batch_size)\n",
    "\n",
    "emissions = tracker.stop()\n",
    "\n",
    "print(f\"Emissions: {emissions} kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "11ca36fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REP ['rintɑ', 'liːtːæː', 'liːkːuɑ', 'terʋeys', 'kɑtːo', 'tuo̯mi', 'ei̯', 'tolɑ', 'rɑu̯tɑ', 'nɑpɑ', 'luo̯to', 'lɑu̯kːu', 'meri', 'luʋɑtɑ', 'kuːnːelːɑ', 'nie̯lːæ', 'kuo̯ri', 'ɑlkɑː', 'ʋene', 'kæsiʋɑrsi', 'syʋæ', 'niskɑ', 'syli', 'oksɑ', 'lisæːntyæ', 'hɑlːitɑ', 'ihme', 'kolme', 'tunːistɑː', 'kɑlɑstɑː', 'iloi̯nen', 'suo̯mu', 'mikæ', 'hei̯lutːɑː', 'muː', 'jɑ', 'noi̯tɑ', 'isæ', 'kirjɑʋɑ', 'kuːsi', 'jæː', 'juo̯dɑ', 'rɑu̯hɑ', 'læmsæ', 'jæːtyæ', 'syntyæ', 'ei̯ koskɑːn', 'ʋɑlmistɑː', 'mɑːilmɑ', 'sæːski', 'sɑlko', 'elæmæ', 'nenæ', 'ɑntɑː', 'ʋuo̯ri', 'ʋiholːinen', 'kɑt͡soɑ', 'pyhæ', 'et͡siæ', 'heʋonen', 'onsi', 'pyː', 'keskus', 'uːtinen', 'tosi', 'kynsi', 'pɑi̯stɑː', 'tæstæ', 'lɑu̯tɑ', 'pilʋi']\n",
      "CAN ['rɑntɑ', 'liːtɑː', 'liːkɑtɑ', 'tæris', 'kɑto', 'tomi', 'ijo', 'tɑlɑ', 'rɑnɑtɑ', 'næi̯n', 'lutɑ', 'lɑi̯ne', 'mere', 'lɑpːɑtɑ', 'kulːɑ', 'nelæː', 'kuri', 'ɑlkɑː', 'ʋinen', 'kerhenen', 'tyæ', 'neskæ', 'suli', 'okɑs', 'liːɑtɑ', 'hiltɑ', 'omistɑ', 'kulmi', 'tuntuɑ', 'kɑlstɑː', 'ilo', 'somi', 'min', 'hei̯lytːæː', 'ninæ', 'jo', 'niːtɑ', 'setæ', 'kirjɑ', 'kusi', 'jæŋkæ', 'juo̯dɑ', 'rɑu̯kɑ', 'lɑi̯pɑlɑ', 'joi̯dɑ', 'soi̯tɑː', 'kentæ', 'ʋɑlmistɑː', 'milmæ', 'sokis', 'sɑlkɑ', 'elæmæ', 'nuo̯ni', 'ʋɑntɑː', 'ʋɑrjɑ', 'ʋilo', 'kestæː', 'pɑi̯s', 'ostuɑ', 'hɑi̯ʋɑ', 'ʋɑni̯ne', 'pɑu̯kɑ', 'keski', 'osu', 'tuhti', 'kynsi', 'pɑtɑ', 'tæhti', 'lɑu̯tɑ', 'pulʋi']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:53:58] Energy consumed for RAM : 0.056893 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 12:53:58] Energy consumed for all CPUs : 0.330831 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 12:53:58] 0.387724 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:54:01] Energy consumed for RAM : 0.056598 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 12:54:01] Energy consumed for all CPUs : 0.329115 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 12:54:01] 0.385713 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:54:13] Energy consumed for RAM : 0.056903 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 12:54:13] Energy consumed for all CPUs : 0.330889 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 12:54:13] 0.387792 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:54:16] Energy consumed for RAM : 0.056608 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 12:54:16] Energy consumed for all CPUs : 0.329173 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 12:54:16] 0.385781 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "r_list=[]\n",
    "c_list=[]\n",
    "r_word_list = []\n",
    "c_word_list = []\n",
    "for pair in test_data:\n",
    "        pair = (torch.unsqueeze(pair[0], 0), torch.unsqueeze(pair[1], 0))\n",
    "        \n",
    "        output_chars, _ = evaluater(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        \n",
    "        in1 = []\n",
    "        for idx in torch.squeeze(pair[1], 0):\n",
    "            if idx.item() == EOS_token:\n",
    "                break\n",
    "            in1.append(output_lang.index2char[idx.item()])\n",
    "\n",
    "        r = ''.join(in1)\n",
    "        c = ''.join(output_chars[:-1])\n",
    "\n",
    "        r_list.append([ch for ch in r])\n",
    "        c_list.append([ch for ch in c])\n",
    "        r_word_list.append(r)\n",
    "        c_word_list.append(c)\n",
    "print('REP', r_word_list)\n",
    "print('CAN', c_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9120c037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> rɑdːe<EOS>\n",
      "= rintɑ<EOS>\n",
      "< rɑntɑ<EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30505/447742087.py:29: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + [*input_word] + ['<EOS>'], rotation=90)\n",
      "/tmp/ipykernel_30505/447742087.py:30: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + output_chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> lɑhteð<EOS>\n",
      "= liːtːæː<EOS>\n",
      "< liːtɑː<EOS>\n",
      "\n",
      "> lihɐdið<EOS>\n",
      "= liːkːuɑ<EOS>\n",
      "< liːkɑtɑ<EOS>\n",
      "\n",
      "> tiervɐsvuotɐ<EOS>\n",
      "= terʋeys<EOS>\n",
      "< tæris<EOS>\n",
      "\n",
      "> kætːu<EOS>\n",
      "= kɑtːo<EOS>\n",
      "< kɑto<EOS>\n",
      "\n",
      "> tuomɐ<EOS>\n",
      "= tuo̯mi<EOS>\n",
      "< tomi<EOS>\n",
      "\n",
      "> ij<EOS>\n",
      "= ei̯<EOS>\n",
      "< ijo<EOS>\n",
      "\n",
      "> tu̯æli<EOS>\n",
      "= tolɑ<EOS>\n",
      "< tɑlɑ<EOS>\n",
      "\n",
      "> ryevdi<EOS>\n",
      "= rɑu̯tɑ<EOS>\n",
      "< rɑnɑtɑ<EOS>\n",
      "\n",
      "> næːpi<EOS>\n",
      "= nɑpɑ<EOS>\n",
      "< næi̯n<EOS>\n",
      "\n",
      "> lu̯ætu<EOS>\n",
      "= luo̯to<EOS>\n",
      "< lutɑ<EOS>\n",
      "\n",
      "> lɑvkːɐ<EOS>\n",
      "= lɑu̯kːu<EOS>\n",
      "< lɑi̯ne<EOS>\n",
      "\n",
      "> meːrɐ<EOS>\n",
      "= meri<EOS>\n",
      "< mere<EOS>\n",
      "\n",
      "> lopedið<EOS>\n",
      "= luʋɑtɑ<EOS>\n",
      "< lɑpːɑtɑ<EOS>\n",
      "\n",
      "> kuldɐlið<EOS>\n",
      "= kuːnːelːɑ<EOS>\n",
      "< kulːɑ<EOS>\n",
      "\n",
      "> ɲielːɐð<EOS>\n",
      "= nie̯lːæ<EOS>\n",
      "< nelæː<EOS>\n",
      "\n",
      "> korːɐ<EOS>\n",
      "= kuo̯ri<EOS>\n",
      "< kuri<EOS>\n",
      "\n",
      "> ælɡið<EOS>\n",
      "= ɑlkɑː<EOS>\n",
      "< ɑlkɑː<EOS>\n",
      "\n",
      "> voːnɐs<EOS>\n",
      "= ʋene<EOS>\n",
      "< ʋinen<EOS>\n",
      "\n",
      "> kietɐverdi<EOS>\n",
      "= kæsiʋɑrsi<EOS>\n",
      "< kerhenen<EOS>\n",
      "\n",
      "> tɑve<EOS>\n",
      "= syʋæ<EOS>\n",
      "< tyæ<EOS>\n",
      "\n",
      "> niske<EOS>\n",
      "= niskɑ<EOS>\n",
      "< neskæ<EOS>\n",
      "\n",
      "> solːɐ<EOS>\n",
      "= syli<EOS>\n",
      "< suli<EOS>\n",
      "\n",
      "> u̯æksi<EOS>\n",
      "= oksɑ<EOS>\n",
      "< okɑs<EOS>\n",
      "\n",
      "> lɑsɑnið<EOS>\n",
      "= lisæːntyæ<EOS>\n",
      "< liːɑtɑ<EOS>\n",
      "\n",
      "> hɑldɐʃið<EOS>\n",
      "= hɑlːitɑ<EOS>\n",
      "< hiltɑ<EOS>\n",
      "\n",
      "> oːmɐs<EOS>\n",
      "= ihme<EOS>\n",
      "< omistɑ<EOS>\n",
      "\n",
      "> kulmɐ<EOS>\n",
      "= kolme<EOS>\n",
      "< kulmi<EOS>\n",
      "\n",
      "> tubdɐð<EOS>\n",
      "= tunːistɑː<EOS>\n",
      "< tuntuɑ<EOS>\n",
      "\n",
      "> ku̯ælæstið<EOS>\n",
      "= kɑlɑstɑː<EOS>\n",
      "< kɑlstɑː<EOS>\n",
      "\n",
      "> iːloːʃ<EOS>\n",
      "= iloi̯nen<EOS>\n",
      "< ilo<EOS>\n",
      "\n",
      "> t͡ʃuomɐ<EOS>\n",
      "= suo̯mu<EOS>\n",
      "< somi<EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:15:08] Energy consumed for RAM : 0.124853 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 17:15:08] Energy consumed for all CPUs : 0.726019 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 17:15:08] Energy consumed for RAM : 0.125150 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 17:15:08] 0.850872 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:15:08] Energy consumed for all CPUs : 0.727744 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 17:15:08] 0.852895 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> miː<EOS>\n",
      "= mikæ<EOS>\n",
      "< min<EOS>\n",
      "\n",
      "> hi̯æilutːið<EOS>\n",
      "= hei̯lutːɑː<EOS>\n",
      "< hei̯lytːæː<EOS>\n",
      "\n",
      "> nubːe<EOS>\n",
      "= muː<EOS>\n",
      "< ninæ<EOS>\n",
      "\n",
      "> jæ<EOS>\n",
      "= jɑ<EOS>\n",
      "< jo<EOS>\n",
      "\n",
      "> nu̯æidi<EOS>\n",
      "= noi̯tɑ<EOS>\n",
      "< niːtɑ<EOS>\n",
      "\n",
      "> eːt͡ʃi<EOS>\n",
      "= isæ<EOS>\n",
      "< setæ<EOS>\n",
      "\n",
      "> kirjæː<EOS>\n",
      "= kirjɑʋɑ<EOS>\n",
      "< kirjɑ<EOS>\n",
      "\n",
      "> kuosɐ<EOS>\n",
      "= kuːsi<EOS>\n",
      "< kusi<EOS>\n",
      "\n",
      "> jieŋɐ<EOS>\n",
      "= jæː<EOS>\n",
      "< jæŋkæ<EOS>\n",
      "\n",
      "> juːḥɐð<EOS>\n",
      "= juo̯dɑ<EOS>\n",
      "< juo̯dɑ<EOS>\n",
      "\n",
      "> ræːvhu<EOS>\n",
      "= rɑu̯hɑ<EOS>\n",
      "< rɑu̯kɑ<EOS>\n",
      "\n",
      "> læbd͡ʒi<EOS>\n",
      "= læmsæ<EOS>\n",
      "< lɑi̯pɑlɑ<EOS>\n",
      "\n",
      "> ji̯æŋːuð<EOS>\n",
      "= jæːtyæ<EOS>\n",
      "< joi̯dɑ<EOS>\n",
      "\n",
      "> ʃodːɐð<EOS>\n",
      "= syntyæ<EOS>\n",
      "< soi̯tɑː<EOS>\n",
      "\n",
      "> ij ku̯æssin<EOS>\n",
      "= ei̯ koskɑːn<EOS>\n",
      "< kentæ<EOS>\n",
      "\n",
      "> vɑlmɐʃtið<EOS>\n",
      "= ʋɑlmistɑː<EOS>\n",
      "< ʋɑlmistɑː<EOS>\n",
      "\n",
      "> mɑːilm<EOS>\n",
      "= mɑːilmɑ<EOS>\n",
      "< milmæ<EOS>\n",
      "\n",
      "> t͡ʃuoʃkɐ<EOS>\n",
      "= sæːski<EOS>\n",
      "< sokis<EOS>\n",
      "\n",
      "> t͡ʃu̯ælɡui<EOS>\n",
      "= sɑlko<EOS>\n",
      "< sɑlkɑ<EOS>\n",
      "\n",
      "> eːlːim<EOS>\n",
      "= elæmæ<EOS>\n",
      "< elæmæ<EOS>\n",
      "\n",
      "> ɲune<EOS>\n",
      "= nenæ<EOS>\n",
      "< nuo̯ni<EOS>\n",
      "\n",
      "> vyebdið<EOS>\n",
      "= ɑntɑː<EOS>\n",
      "< ʋɑntɑː<EOS>\n",
      "\n",
      "> væːri<EOS>\n",
      "= ʋuo̯ri<EOS>\n",
      "< ʋɑrjɑ<EOS>\n",
      "\n",
      "> vɑjɑlɐʃ<EOS>\n",
      "= ʋiholːinen<EOS>\n",
      "< ʋilo<EOS>\n",
      "\n",
      "> ket͡ʃːɐð<EOS>\n",
      "= kɑt͡soɑ<EOS>\n",
      "< kestæː<EOS>\n",
      "\n",
      "> pɑse<EOS>\n",
      "= pyhæ<EOS>\n",
      "< pɑi̯s<EOS>\n",
      "\n",
      "> uːt͡sːɐð<EOS>\n",
      "= et͡siæ<EOS>\n",
      "< ostuɑ<EOS>\n",
      "\n",
      "> hi̯ævuʃ<EOS>\n",
      "= heʋonen<EOS>\n",
      "< hɑi̯ʋɑ<EOS>\n",
      "\n",
      "> vuobdɐ<EOS>\n",
      "= onsi<EOS>\n",
      "< ʋɑni̯ne<EOS>\n",
      "\n",
      "> pu̯ægui<EOS>\n",
      "= pyː<EOS>\n",
      "< pɑu̯kɑ<EOS>\n",
      "\n",
      "> koskɐ<EOS>\n",
      "= keskus<EOS>\n",
      "< keski<EOS>\n",
      "\n",
      "> uːðɐs<EOS>\n",
      "= uːtinen<EOS>\n",
      "< osu<EOS>\n",
      "\n",
      "> tuotɐ<EOS>\n",
      "= tosi<EOS>\n",
      "< tuhti<EOS>\n",
      "\n",
      "> kod͡zːɐ<EOS>\n",
      "= kynsi<EOS>\n",
      "< kynsi<EOS>\n",
      "\n",
      "> pɑsːeːð<EOS>\n",
      "= pɑi̯stɑː<EOS>\n",
      "< pɑtɑ<EOS>\n",
      "\n",
      "> tæstæn<EOS>\n",
      "= tæstæ<EOS>\n",
      "< tæhti<EOS>\n",
      "\n",
      "> lyevdi<EOS>\n",
      "= lɑu̯tɑ<EOS>\n",
      "< lɑu̯tɑ<EOS>\n",
      "\n",
      "> polvɐ<EOS>\n",
      "= pilʋi<EOS>\n",
      "< pulʋi<EOS>\n",
      "\n",
      "> lyevdi<EOS>\n",
      "= lɑu̯tɑ<EOS>\n",
      "< lɑu̯tɑ<EOS>\n",
      "\n",
      "> vuoiɡɐ<EOS>\n",
      "= oi̯keɑ<EOS>\n",
      "< ʋɑi̯keɑ<EOS>\n",
      "\n",
      "> vu̯ælus<EOS>\n",
      "= ɑlɑs<EOS>\n",
      "< ɑlɑs<EOS>\n",
      "\n",
      "> meːri<EOS>\n",
      "= mæːræ<EOS>\n",
      "< meræ<EOS>\n",
      "\n",
      "> voːnɐs<EOS>\n",
      "= ʋene<EOS>\n",
      "< ʋinen<EOS>\n",
      "\n",
      "> koijɐdið<EOS>\n",
      "= kysyæ<EOS>\n",
      "< kiːtɑː<EOS>\n",
      "\n",
      "> æʃːi<EOS>\n",
      "= ɑsiɑ<EOS>\n",
      "< ɑsiɑ<EOS>\n",
      "\n",
      "> lɑvkːɐ<EOS>\n",
      "= lɑu̯kːu<EOS>\n",
      "< lɑi̯ne<EOS>\n",
      "\n",
      "> juo<EOS>\n",
      "= jo<EOS>\n",
      "< jo<EOS>\n",
      "\n",
      "> ki̯æstu<EOS>\n",
      "= kimpːu<EOS>\n",
      "< kestæ<EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:15:23] Energy consumed for RAM : 0.124863 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 17:15:23] Energy consumed for all CPUs : 0.726078 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 17:15:23] 0.850941 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:15:23] Energy consumed for RAM : 0.125160 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 17:15:23] Energy consumed for all CPUs : 0.727803 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 17:15:23] 0.852963 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:15:38] Energy consumed for RAM : 0.124873 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 17:15:38] Energy consumed for all CPUs : 0.726136 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 17:15:38] Energy consumed for RAM : 0.125170 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 17:15:38] 0.851009 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:15:38] Energy consumed for all CPUs : 0.727861 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 17:15:38] 0.853031 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:15:53] Energy consumed for RAM : 0.124883 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 17:15:53] Energy consumed for all CPUs : 0.726194 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 17:15:53] Energy consumed for RAM : 0.125180 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 17:15:53] 0.851077 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:15:53] Energy consumed for all CPUs : 0.727919 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 17:15:53] 0.853100 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:16:08] Energy consumed for RAM : 0.124893 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 17:16:08] Energy consumed for all CPUs : 0.726253 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 17:16:08] Energy consumed for RAM : 0.125190 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 17:16:08] 0.851146 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:16:08] Energy consumed for all CPUs : 0.727978 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 17:16:08] 0.853168 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:16:23] Energy consumed for RAM : 0.124903 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 17:16:23] Energy consumed for all CPUs : 0.726311 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 17:16:23] Energy consumed for RAM : 0.125200 kWh. RAM Power : 2.4078168869018555 W\n",
      "[codecarbon INFO @ 17:16:23] 0.851214 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:16:23] Energy consumed for all CPUs : 0.728036 kWh. Total CPU Power : 14.0 W\n",
      "[codecarbon INFO @ 17:16:23] 0.853236 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder, eval_all=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
